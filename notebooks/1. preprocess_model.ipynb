{"cells":[{"cell_type":"markdown","metadata":{"id":"LkB2On7Mv5yY"},"source":["# This is the Sinapse group's notebook - predictive model about PowerCo's churn."]},{"cell_type":"markdown","metadata":{"id":"dDEfZTrQVTmE"},"source":["This notebook contains:\n","- data exploration\n","- preprocessing\n","- modeling\n","- metrics\n","- analysis about the problem"]},{"cell_type":"markdown","metadata":{"id":"4exb-ZU1dqlb"},"source":["# Installation of libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"QWruHYhshXNb","outputId":"4f66f85a-8f08-4a78-e1cd-396f16353577"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed scikit-learn-1.3.1\n","Collecting klib\n","  Downloading klib-1.1.2-py3-none-any.whl (22 kB)\n","Requirement already satisfied: Jinja2<4.0.0,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from klib) (3.1.2)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from klib) (3.7.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from klib) (1.23.5)\n","Requirement already satisfied: pandas<3.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from klib) (1.5.3)\n","Requirement already satisfied: plotly<6.0.0,>=5.2.2 in /usr/local/lib/python3.10/dist-packages (from klib) (5.15.0)\n","Requirement already satisfied: scipy<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from klib) (1.11.3)\n","Collecting screeninfo<0.9.0,>=0.8.1 (from klib)\n","  Downloading screeninfo-0.8.1-py3-none-any.whl (12 kB)\n","Requirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.10/dist-packages (from klib) (0.12.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4.0.0,>=3.0.3->klib) (2.1.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (23.2)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0.0,>=3.0.3->klib) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.2->klib) (2023.3.post1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly<6.0.0,>=5.2.2->klib) (8.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.0.3->klib) (1.16.0)\n","Installing collected packages: screeninfo, klib\n","Successfully installed klib-1.1.2 screeninfo-0.8.1\n"]}],"source":["# Instalação e atualização de bibliotecas: scikit-learn e klib\n","!pip install --upgrade scikit-learn\n","!pip install klib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQaeNGPmLVCx"},"outputs":[],"source":["# Installation of libraries\n","import math\n","import pandas as pd\n","import numpy as np\n","import klib\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from os import replace\n","from sklearn.linear_model import LinearRegression"]},{"cell_type":"markdown","metadata":{"id":"___gsmJBdwnu"},"source":["# Importing tables from google drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y01D9iFxMmNz","outputId":"4ffd2855-caac-4761-c4c1-5638b3770589"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1TLD79damXSqYHkUUMU754HUmKkYnK1jG\n","To: /content/base_clientes.csv\n","\r  0% 0.00/5.25M [00:00<?, ?B/s]\r100% 5.25M/5.25M [00:00<00:00, 197MB/s]\n"]}],"source":["#import from google drive\n","!gdown 1TLD79damXSqYHkUUMU754HUmKkYnK1jG\n","\n","!gdown 1Nza3vUOL7WalKPG32BVQrso7J_Oeqj6x\n","\n","!gdown 18B3pWhLVitH8H7Vaq88py19aaT2TqSeZ"]},{"cell_type":"markdown","metadata":{"id":"p1BBx42Ld3qq"},"source":["# Defining the dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApOr0k0nNmnB"},"outputs":[],"source":["#important and defining the dataframes\n","data_customers = pd.read_csv('base_clientes.csv')\n","\n","data_prices = pd.read_csv('base_precos.csv')\n","\n","data_churns = pd.read_csv('base_hist_churn.csv', ',')\n","\n","pd.set_option('display.max_columns', None)"]},{"cell_type":"markdown","metadata":{"id":"qnokvEAGsoKF"},"source":["# Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otitqHU6sraj"},"outputs":[],"source":["# This function is used to handle date data, ensuring it follows standard conventions.\n","def to_dt(cell):\n","  try:\n","    if pd.isnull(cell):\n","        return cell\n","    elif int(cell.split('/')[0]) == 29 and int(cell.split('/')[1]) == 2:\n","        return cell.replace('29/02', '28/02')\n","    else:\n","        return cell\n","  except:\n","    return cell"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3FjHI5SBdRN"},"outputs":[],"source":["# Calculates consumption by dividing 'fix_price' by 'var_price' (avoiding division by zero), used for cost analysis.\n","def calculate_consumption(fix_price, var_price):\n","    if var_price != 0:\n","        return fix_price / var_price\n","    else:\n","        return 0"]},{"cell_type":"markdown","metadata":{"id":"nv9Oi3PtjVd2"},"source":["# Data Customers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGgaV86ljSoM"},"outputs":[],"source":["# Displays information about the 'data_customers' DataFrame, including data types, non-null counts, and memory usage\n","data_customers.info()"]},{"cell_type":"markdown","metadata":{"id":"K5Fjq4pOk83I"},"source":["We will use our filled column to populate the *forecast_base_bill_ele* column via linear regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJC4pg93kFnm"},"outputs":[],"source":["# Utilizes linear regression to impute missing values in 'forecast_base_bill_ele' and 'forecast_base_bill_year'.\n","regression_data = data_customers[['forecast_base_bill_ele', 'imp_cons']].dropna()\n","\n","X = regression_data[['imp_cons']]\n","y = regression_data['forecast_base_bill_ele']\n","\n","model_fill = LinearRegression()\n","model_fill.fit(X, y)\n","\n","coef = model_fill.coef_[0]\n","intercept = model_fill.intercept_\n","\n","data_customers['forecast_base_bill_ele'].fillna(data_customers['imp_cons'] * coef + intercept, inplace=True)\n","data_customers['forecast_base_bill_year'].fillna(data_customers['imp_cons'] * coef + intercept, inplace=True)\n","\n","missing = data_customers[['forecast_base_bill_ele', 'forecast_base_bill_year']].isnull().sum()\n","missing"]},{"cell_type":"markdown","metadata":{"id":"tJw1YPqfnB0t"},"source":["As the *has_gas* column is like t and f, just change this to 0 and 1 (a more interpretable way for the models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDZnyV55lnUu"},"outputs":[],"source":["# Converts the 'has_gas' column values from 't' and 'f' to 1 and 0, respectively, and changes the data type to integer.\n","data_customers['has_gas'] = data_customers['has_gas'].map({'t': 1, 'f': 0})\n","data_customers['has_gas'] = data_customers['has_gas'].astype(int)"]},{"cell_type":"markdown","metadata":{"id":"YYaHVPYunX3f"},"source":["In the database, there are 3 columns defined by codes *channel_sales*, *origin_up*, *activity_new*:\n","For a correct interpretation of the models, these values must be converted to numbers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEF2BRL-n8VG"},"outputs":[],"source":["# Replaces NaN values in the 'channel_sales' column with 'no_fill', then creates dummy variables for 'channel_sales'.\n","data_customers['channel_sales'] = data_customers['channel_sales'].replace(np.nan, 'no_fill')\n","data_customers = pd.get_dummies(data_customers, columns=['channel_sales'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbV3r3R1oMTt"},"outputs":[],"source":["# Replaces NaN values in the 'origin_up' column with 'no_fill', then creates dummy variables for 'origin_up'.\n","data_customers['origin_up'] = data_customers['origin_up'].replace(np.nan, 'no_fill')\n","data_customers = pd.get_dummies(data_customers, columns=['origin_up'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bmBrjZrpoOY9"},"outputs":[],"source":["# Replaces NaN values in the 'activity_new' column with 'no_fill' and converts it to binary values (1 for not NaN, 0 for NaN)\n","data_customers['activity_new'] = data_customers['activity_new'].replace(np.nan, 'no_fill')\n","data_customers['activity_new'] = data_customers['activity_new'].notna().astype(int)"]},{"cell_type":"markdown","metadata":{"id":"XYoh8aFzpynG"},"source":["You can see a column with all null values, which gives you the option to discard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y5l2vtInpTrc"},"outputs":[],"source":["# Drops the 'campaign_disc_ele' column from the 'data_customers' DataFrame\n","data_customers.drop(['campaign_disc_ele'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"bWi1fjIHquvL"},"source":["Após uma análise das colunas importantes, definimos quais são as colunas que queremos no final do tratamento, por isso, retiramos as colunas : *forecast_base_bill_year*, *forecast_bill_12m*, *forecast_cons* e *date_first_activ*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnLfQYiSqkwd"},"outputs":[],"source":["# Drops multiple columns ('forecast_base_bill_year', 'forecast_bill_12m', 'forecast_cons', 'date_first_activ')\n","data_customers.drop(['forecast_base_bill_year', 'forecast_bill_12m', 'forecast_cons', 'date_first_activ'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"eL5PUvjBroxh"},"source":["The next step is to treat the dates in the table, a quick analysis allows you to discover that they have invalid dates, so it is necessary to treat these dates. The way to treat them can be either through cyclic functions or by putting the year to represent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuHyqlODsjGc"},"outputs":[],"source":["# Applies the 'to_dt' function to convert date columns to a consistent format and then converts them to datetime objects.\n","data_customers['date_end'] = data_customers['date_end'].apply(to_dt)\n","data_customers['date_end'] = pd.to_datetime(data_customers['date_end'])\n","\n","data_customers['date_activ'] = data_customers['date_activ'].apply(to_dt)\n","data_customers['date_activ'] = pd.to_datetime(data_customers['date_activ'])\n","\n","data_customers['date_modif_prod'] = data_customers['date_modif_prod'].apply(to_dt)\n","data_customers['date_modif_prod'] = pd.to_datetime(data_customers['date_modif_prod'])\n","\n","data_customers['date_renewal'] = data_customers['date_renewal'].apply(to_dt)\n","data_customers['date_renewal'] = pd.to_datetime(data_customers['date_renewal'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA60yUbZuC88"},"outputs":[],"source":["# Converts date columns to cyclical representations using sine and cosine transformations, dropping the original date columns.\n","data_customers['dia_do_ano'] = data_customers['date_end'].dt.dayofyear\n","data_customers['date_end_sin'] = np.sin(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers['date_end_cos'] = np.cos(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers.drop(['date_end', 'dia_do_ano'], axis=1, inplace=True)\n","\n","data_customers['dia_do_ano'] = data_customers['date_activ'].dt.dayofyear\n","data_customers['date_activ_sin'] = np.sin(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers['dia_activ_cos'] = np.cos(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers.drop(['date_activ', 'dia_do_ano'], axis=1, inplace=True)\n","\n","data_customers['dia_do_ano'] = data_customers['date_modif_prod'].dt.dayofyear\n","data_customers['date_modif_prod_sin'] = np.sin(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers['date_modif_prod_cos'] = np.cos(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers.drop(['date_modif_prod', 'dia_do_ano'], axis=1, inplace=True)\n","\n","data_customers['dia_do_ano'] = data_customers['date_renewal'].dt.dayofyear\n","data_customers['date_renewal_sin'] = np.sin(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers['date_renewal_cos'] = np.cos(2 * np.pi * data_customers['dia_do_ano']/365)\n","data_customers.drop(['date_renewal', 'dia_do_ano'], axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"XpcEB0GKvy59"},"source":["Columns that have negative values were identified, for the model, they do not make sense, so we will remove these negative values from the table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBLdxIPau8Ki"},"outputs":[],"source":["# Filters 'data_customers' DataFrame to retain rows where all specified columns in 'negatives' have non-negative values.\n","negatives = [\n","    'cons_12m', 'cons_gas_12m', 'cons_last_month', 'forecast_cons_12m',\n","    'forecast_cons_year', 'forecast_base_bill_ele','forecast_meter_rent_12m', 'forecast_price_pow_p1',\n","    'imp_cons', 'net_margin'\n","]\n","\n","data_customers = data_customers[(data_customers[negatives] >= 0).all(axis=1)].reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"nuuh86wXxLpp"},"source":["To ensure that all values in the *margin_net_pow_ele*, *margin_gross_pow_ele* columns are treated as positive values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fueR2TRKxLQa"},"outputs":[],"source":["# Takes the absolute values of 'margin_net_pow_ele' and 'margin_gross_pow_ele' columns\n","data_customers['margin_net_pow_ele'] = data_customers['margin_net_pow_ele'].abs()\n","data_customers['margin_gross_pow_ele'] = data_customers['margin_gross_pow_ele'].abs()"]},{"cell_type":"markdown","metadata":{"id":"6GFd9QZ-zUsP"},"source":["To ensure that the database is as clean as possible to continue the feature engineering process, we must perform a cleaning and memory reduction process"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0fX9daz2YJn"},"outputs":[],"source":["# Performs data cleaning on the 'data_customers' DataFrame using the klib library's data_cleaning function.\n","data_customers = klib.data_cleaning(data_customers)"]},{"cell_type":"markdown","metadata":{"id":"9HRXwN654Cq4"},"source":["# Data Prices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22UXj3TO4JKg"},"outputs":[],"source":["# Displays information about the 'data_prices' DataFrame, including data types, non-null counts, and memory usage.\n","data_prices.info()"]},{"cell_type":"markdown","metadata":{"id":"B6LvCMhBAcxf"},"source":["First, we can remove the *price_date* column, because it will not be useful for interpreting the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlH_wE8qAt-J"},"outputs":[],"source":["# Removes the 'price_date' column from 'data_prices' DataFrame and drops duplicate rows, keeping the first occurrence.\n","data_prices = data_prices.drop('price_date', axis=1).drop_duplicates()"]},{"cell_type":"markdown","metadata":{"id":"zbX3CvXz4ps3"},"source":["It can be seen that in this table, there are 3 periods that can be treated as sets. This means we can make new columns that include the average of the other columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v01lUFVx4kJu"},"outputs":[],"source":["# Calculates the mean of the columns 'price_p1_var', 'price_p2_var', and 'price_p3_var' and stores the result in 'price_var_mean'.\n","data_prices['price_var_mean'] = data_prices[['price_p1_var', 'price_p2_var', 'price_p3_var']].mean(axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXpHbYPZ5T2S"},"outputs":[],"source":["# Calculates the mean of the columns 'price_p1_fix', 'price_p2_fix', and 'price_p3_fix' and stores the result in 'price_fix_mean'.\n","data_prices['price_fix_mean'] = data_prices[['price_p1_fix', 'price_p2_fix', 'price_p3_fix']].mean(axis=1)"]},{"cell_type":"markdown","metadata":{"id":"23PJwo2BDimk"},"source":["As the table has more than two hundred thousand rows, we have to group it in some way, in this case we will group by id and apply the median to describe the values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2eo4WCep-uSd"},"outputs":[],"source":["# Groups 'data_prices' by 'id' and calculates the median for specified columns, resetting the index in the resulting DataFrame.\n","data_prices = data_prices.groupby('id').agg({\n","    'price_p1_var': 'median',\n","    'price_p2_var': 'median',\n","    'price_p3_var': 'median',\n","    'price_p1_fix': 'median',\n","    'price_p2_fix': 'median',\n","    'price_p3_fix': 'median',\n","    'price_var_mean': 'median',\n","    'price_fix_mean': 'median'\n","}).reset_index()"]},{"cell_type":"markdown","metadata":{"id":"lPMb8mWFDIIW"},"source":["Now, let's create descriptive columns that bring a number for the total price per kWh and the price paid, in addition to the customer's consumption as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aactrqCUBbSO"},"outputs":[],"source":["# Computes 'price_var' and 'price_fix' as sums of corresponding price columns, calculates 'consumption', and drops original price columns.\n","data_prices['price_var'] = data_prices[['price_p1_var', 'price_p2_var', 'price_p3_var']].sum(axis=1)\n","data_prices['price_fix'] = data_prices[['price_p1_fix', 'price_p2_fix', 'price_p3_fix']].sum(axis=1)\n","\n","data_prices['consumption'] = data_prices['price_fix'] / data_prices['price_var']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yoreEWXTCNwK"},"outputs":[],"source":["# Resets the index of the 'data_prices' DataFrame, dropping the previous index and keeping it dropped.\n","data_prices.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"CeMx1rq3Dylp"},"source":["Finally, let's clean this dataframe to make it lighter for the processes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1e-XEIgpD6SF"},"outputs":[],"source":["# Uses the 'klib' library's data cleaning function to perform data cleaning operations on 'data_prices' DataFrame.\n","data_prices = klib.data_cleaning(data_prices)"]},{"cell_type":"markdown","metadata":{"id":"5bw0mzMhEFyh"},"source":["# Tables Merge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HcqW63kYEK7s"},"outputs":[],"source":["# Performs an inner merge between 'data_customers' and 'data_prices' DataFrames based on the 'id' column, creating 'df_final'.\n","df_final = data_customers.merge(data_prices, how='inner', on='id')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pC_HjaVYHK7n"},"outputs":[],"source":["# Fills any remaining missing values in 'df_final' with zeros.\n","df_final = df_final.fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ju3dr9EkE0Ur"},"outputs":[],"source":["# Checks and counts the number of missing values in each column of the 'df_final' DataFrame.\n","df_final.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"di-BD7SSIm6W"},"outputs":[],"source":["# Performs an inner merge between 'df_final' and 'data_churns' DataFrames based on the 'id' column, updating 'df_final'.\n","df_final = pd.merge(df_final, data_churns, on='id', how='inner')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4E6mxKdmHtjH"},"outputs":[],"source":["# Applying the data_cleaning function from the klib library to the df_final DataFrame\n","df_final = klib.data_cleaning(df_final)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVAX26MPgAbR"},"outputs":[],"source":["# Save the DataFrame df_final to a CSV file\n","df_final.to_csv('/content/df_final.csv')"]}],"metadata":{"colab":{"collapsed_sections":["LkB2On7Mv5yY","4exb-ZU1dqlb","___gsmJBdwnu","p1BBx42Ld3qq","qnokvEAGsoKF","nv9Oi3PtjVd2","9HRXwN654Cq4","5bw0mzMhEFyh"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}